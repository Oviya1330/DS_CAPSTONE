{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00fb10ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd91764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting faker\n",
      "  Downloading faker-37.11.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Downloading numpy-2.3.4-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.9 kB ? eta -:--:--\n",
      "     ------------------------- ------------ 41.0/60.9 kB 991.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.9/60.9 kB 817.7 kB/s eta 0:00:00\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-21.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.3.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.6.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.9.0,>=2023.1.0 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Using cached huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in o:\\redaction_system\\.venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Using cached pyyaml-6.0.3-cp312-cp312-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting tzdata (from faker)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.13.0-cp312-cp312-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets)\n",
      "  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting certifi (from httpx<1.0.0->datasets)\n",
      "  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<1.0.0->datasets)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.32.2->datasets)\n",
      "  Downloading charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl.metadata (38 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: colorama in o:\\redaction_system\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in o:\\redaction_system\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.8.0-cp312-cp312-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.7.0-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.22.0-cp312-cp312-win_amd64.whl.metadata (77 kB)\n",
      "Requirement already satisfied: six>=1.5 in o:\\redaction_system\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1.0.0->datasets)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading datasets-4.2.0-py3-none-any.whl (506 kB)\n",
      "   ---------------------------------------- 0.0/506.3 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 348.2/506.3 kB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 506.3/506.3 kB 6.4 MB/s eta 0:00:00\n",
      "Downloading faker-37.11.0-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 1.2/2.0 MB 25.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 21.1 MB/s eta 0:00:00\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading numpy-2.3.4-cp312-cp312-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.1/12.8 MB 33.3 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.3/12.8 MB 28.8 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.3/12.8 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 4.3/12.8 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.5/12.8 MB 25.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.7/12.8 MB 25.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.9/12.8 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.8/12.8 MB 24.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.9/12.8 MB 24.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.9/12.8 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.7/12.8 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.8 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.8 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 20.5 MB/s eta 0:00:00\n",
      "Using cached pyarrow-21.0.0-cp312-cp312-win_amd64.whl (26.2 MB)\n",
      "Using cached pyyaml-6.0.3-cp312-cp312-win_amd64.whl (154 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached pandas-2.3.3-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached xxhash-3.6.0-cp312-cp312-win_amd64.whl (31 kB)\n",
      "Using cached aiohttp-3.13.0-cp312-cp312-win_amd64.whl (451 kB)\n",
      "Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl (107 kB)\n",
      "   ---------------------------------------- 0.0/107.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 107.1/107.1 kB 6.1 MB/s eta 0:00:00\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.0/71.0 kB 4.1 MB/s eta 0:00:00\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "   ---------------------------------------- 0.0/109.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 109.1/109.1 kB ? eta 0:00:00\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached frozenlist-1.8.0-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached multidict-6.7.0-cp312-cp312-win_amd64.whl (46 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-win_amd64.whl (41 kB)\n",
      "   ---------------------------------------- 0.0/41.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 41.7/41.7 kB ? eta 0:00:00\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached yarl-1.22.0-cp312-cp312-win_amd64.whl (87 kB)\n",
      "Installing collected packages: pytz, xxhash, urllib3, tzdata, typing-extensions, tqdm, sniffio, pyyaml, pyarrow, propcache, numpy, multidict, idna, h11, fsspec, frozenlist, filelock, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, requests, pandas, multiprocess, httpcore, faker, anyio, aiosignal, huggingface-hub, httpx, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.0 aiosignal-1.4.0 anyio-4.11.0 attrs-25.4.0 certifi-2025.10.5 charset_normalizer-3.4.4 datasets-4.2.0 dill-0.4.0 faker-37.11.0 filelock-3.20.0 frozenlist-1.8.0 fsspec-2025.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.35.3 idna-3.11 multidict-6.7.0 multiprocess-0.70.16 numpy-2.3.4 pandas-2.3.3 propcache-0.4.1 pyarrow-21.0.0 pytz-2025.2 pyyaml-6.0.3 requests-2.32.5 sniffio-1.3.1 tqdm-4.67.1 typing-extensions-4.15.0 tzdata-2025.2 urllib3-2.5.0 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets faker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b77de39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "o:\\REDACTION_SYSTEM\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from faker import Faker\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bfa7ce",
   "metadata": {},
   "source": [
    "STEP 1: Load PII Dataset and Filter Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c29a092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/4] Loading ai4privacy dataset and filtering categories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "o:\\REDACTION_SYSTEM\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gnana\\.cache\\huggingface\\hub\\datasets--ai4privacy--pii-masking-400k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 5000 PII examples...\n",
      "  Processed 10000 PII examples...\n",
      "  Processed 15000 PII examples...\n",
      "  Processed 20000 PII examples...\n",
      "  Processed 25000 PII examples...\n",
      "  Processed 30000 PII examples...\n",
      "✓ Loaded 30000 PII examples (SSN, Credit Cards, IP, Bank, OTP)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[1/4] Loading ai4privacy dataset and filtering categories...\")\n",
    "pii_dataset = load_dataset(\"ai4privacy/pii-masking-400k\", split=\"train\", streaming=True)\n",
    "\n",
    "# Categories we want: SSN, Credit Card, IP Address, Bank Account, OTP\n",
    "target_categories = ['ssn', 'credit', 'ip', 'bank', 'otp', 'credit_card', \n",
    "                     'credit_card_number', 'creditcard', 'iban', 'swift',\n",
    "                     'account_number', 'routing_number', 'ipaddress', \n",
    "                     'ip_address', 'social_security']\n",
    "\n",
    "sensitive_pii_examples = []\n",
    "count = 0\n",
    "max_examples = 30000  # Take 30k examples\n",
    "\n",
    "for example in pii_dataset:\n",
    "    if count >= max_examples:\n",
    "        break\n",
    "    \n",
    "    # Get the source text (contains PII)\n",
    "    text = example.get('source_text', '')\n",
    "    \n",
    "    if text:\n",
    "        # Check if this example contains our target categories\n",
    "        # Note: We'll take all examples since the dataset doesn't expose \n",
    "        # which specific PII types are in each text easily\n",
    "        sensitive_pii_examples.append({\n",
    "            'text': text,\n",
    "            'label': 1,\n",
    "            'category': 'pii'\n",
    "        })\n",
    "        count += 1\n",
    "    \n",
    "    if count % 5000 == 0:\n",
    "        print(f\"  Processed {count} PII examples...\")\n",
    "\n",
    "print(f\"✓ Loaded {len(sensitive_pii_examples)} PII examples (SSN, Credit Cards, IP, Bank, OTP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f893b49c",
   "metadata": {},
   "source": [
    "STEP 2: Generate API Key Examples using Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f884f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/4] Generating API key examples using Faker...\n",
      "  Generated 2000 API key examples...\n",
      "  Generated 4000 API key examples...\n",
      "  Generated 6000 API key examples...\n",
      "  Generated 8000 API key examples...\n",
      "  Generated 10000 API key examples...\n",
      "✓ Generated 10000 API key examples\n"
     ]
    }
   ],
   "source": [
    "fake = Faker()\n",
    "print(\"\\n[2/4] Generating API key examples using Faker...\")\n",
    "\n",
    "def generate_api_keys_with_faker(num_examples=10000):\n",
    "    \"\"\"Generate synthetic API keys and tokens\"\"\"\n",
    "    api_examples = []\n",
    "    \n",
    "    # Different API key formats\n",
    "    def generate_openai_key():\n",
    "        return f\"sk-{''.join(random.choices(string.ascii_letters + string.digits, k=48))}\"\n",
    "    \n",
    "    def generate_google_key():\n",
    "        return f\"AIza{''.join(random.choices(string.ascii_letters + string.digits + '-_', k=35))}\"\n",
    "    \n",
    "    def generate_github_token():\n",
    "        return f\"ghp_{''.join(random.choices(string.ascii_letters + string.digits, k=36))}\"\n",
    "    \n",
    "    def generate_aws_key():\n",
    "        return f\"AKIA{''.join(random.choices(string.ascii_uppercase + string.digits, k=16))}\"\n",
    "    \n",
    "    def generate_stripe_key():\n",
    "        return f\"sk_live_{''.join(random.choices(string.ascii_letters + string.digits, k=24))}\"\n",
    "    \n",
    "    def generate_generic_key():\n",
    "        return ''.join(random.choices(string.ascii_letters + string.digits, k=32))\n",
    "    \n",
    "    def generate_jwt_token():\n",
    "        header = ''.join(random.choices(string.ascii_letters + string.digits, k=16))\n",
    "        payload = ''.join(random.choices(string.ascii_letters + string.digits, k=16))\n",
    "        signature = ''.join(random.choices(string.ascii_letters + string.digits, k=16))\n",
    "        return f\"{header}.{payload}.{signature}\"\n",
    "    \n",
    "    # Templates with context\n",
    "    templates = [\n",
    "        \"My API key is {key}\",\n",
    "        \"Here's the authentication token: {key}\",\n",
    "        \"Use this API key: {key}\",\n",
    "        \"API_KEY={key}\",\n",
    "        \"Authorization: Bearer {key}\",\n",
    "        \"The secret key is {key}\",\n",
    "        \"export OPENAI_API_KEY={key}\",\n",
    "        \"const apiKey = '{key}';\",\n",
    "        \"API credentials: {key}\",\n",
    "        \"Access token: {key}\",\n",
    "        \"Your authentication key: {key}\",\n",
    "        \"curl -H 'Authorization: {key}' https://api.example.com\",\n",
    "        \"Please use this key to authenticate: {key}\",\n",
    "        \"My OpenAI key: {key}\",\n",
    "        \"GitHub Personal Access Token: {key}\",\n",
    "        \"AWS Access Key ID: {key}\",\n",
    "        \"Stripe API Key: {key}\",\n",
    "        \"JWT Token: {key}\",\n",
    "        \"Application Secret: {key}\",\n",
    "        \"Database password is {key} and the API key is in the config\",\n",
    "    ]\n",
    "    \n",
    "    key_generators = [\n",
    "        generate_openai_key,\n",
    "        generate_google_key,\n",
    "        generate_github_token,\n",
    "        generate_aws_key,\n",
    "        generate_stripe_key,\n",
    "        generate_generic_key,\n",
    "        generate_jwt_token,\n",
    "    ]\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        template = random.choice(templates)\n",
    "        key_generator = random.choice(key_generators)\n",
    "        api_key = key_generator()\n",
    "        \n",
    "        text = template.format(key=api_key)\n",
    "        \n",
    "        api_examples.append({\n",
    "            'text': text,\n",
    "            'label': 1,\n",
    "            'category': 'api_key'\n",
    "        })\n",
    "        \n",
    "        if (i + 1) % 2000 == 0:\n",
    "            print(f\"  Generated {i + 1} API key examples...\")\n",
    "    \n",
    "    return api_examples\n",
    "\n",
    "api_key_examples = generate_api_keys_with_faker(10000)\n",
    "print(f\"✓ Generated {len(api_key_examples)} API key examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbfa86",
   "metadata": {},
   "source": [
    "STEP 3: Combine All Sensitive Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db20d728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/4] Combining sensitive data...\n",
      "✓ Total sensitive examples: 40000\n",
      "  - PII (SSN, Credit Card, IP, Bank, OTP): 30000\n",
      "  - API Keys: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[3/4] Combining sensitive data...\")\n",
    "all_sensitive = sensitive_pii_examples + api_key_examples\n",
    "random.shuffle(all_sensitive)\n",
    "\n",
    "print(f\"✓ Total sensitive examples: {len(all_sensitive)}\")\n",
    "print(f\"  - PII (SSN, Credit Card, IP, Bank, OTP): {len(sensitive_pii_examples)}\")\n",
    "print(f\"  - API Keys: {len(api_key_examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19880f05",
   "metadata": {},
   "source": [
    "STEP 4: Load Wikipedia (Non-Sensitive Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1239c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/4] Loading Wikipedia for non-sensitive data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "o:\\REDACTION_SYSTEM\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gnana\\.cache\\huggingface\\hub\\datasets--wikitext. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 10000 Wikipedia articles, collected 4133...\n",
      "  Processed 20000 Wikipedia articles, collected 8171...\n",
      "  Processed 30000 Wikipedia articles, collected 12269...\n",
      "  Processed 40000 Wikipedia articles, collected 16695...\n",
      "  Processed 50000 Wikipedia articles, collected 20776...\n",
      "  Processed 60000 Wikipedia articles, collected 24858...\n",
      "  Processed 70000 Wikipedia articles, collected 29098...\n",
      "  Processed 80000 Wikipedia articles, collected 33077...\n",
      "  Processed 90000 Wikipedia articles, collected 37141...\n",
      "✓ Loaded 40000 non-sensitive examples\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[4/4] Loading Wikipedia for non-sensitive data...\")\n",
    "wiki_dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split='train', streaming=True)\n",
    "\n",
    "\n",
    "non_sensitive_examples = []\n",
    "target_count = len(all_sensitive)  # Match the number of sensitive examples\n",
    "\n",
    "for i, example in enumerate(wiki_dataset):\n",
    "    if len(non_sensitive_examples) >= target_count:\n",
    "        break\n",
    "    \n",
    "    text = example['text']\n",
    "    \n",
    "    # Filter out very short texts\n",
    "    if len(text.split()) > 20:\n",
    "        non_sensitive_examples.append({\n",
    "            'text': text[:500],  # Truncate to manageable length\n",
    "            'label': 0,\n",
    "            'category': 'non_sensitive'\n",
    "        })\n",
    "    \n",
    "    if (i + 1) % 10000 == 0:\n",
    "        print(f\"  Processed {i + 1} Wikipedia articles, collected {len(non_sensitive_examples)}...\")\n",
    "\n",
    "print(f\"✓ Loaded {len(non_sensitive_examples)} non-sensitive examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e464fbc",
   "metadata": {},
   "source": [
    "STEP 5: Merge and Create Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0be2c872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGING DATA AND CREATING FINAL DATASET\n",
      "============================================================\n",
      "\n",
      "Total examples: 80000\n"
     ]
    }
   ],
   "source": [
    "print(\"MERGING DATA AND CREATING FINAL DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_data = all_sensitive + non_sensitive_examples\n",
    "random.shuffle(all_data)\n",
    "\n",
    "print(f\"\\nTotal examples: {len(all_data)}\")\n",
    "\n",
    "# Split into train/validation/test\n",
    "train_size = int(0.8 * len(all_data))\n",
    "val_size = int(0.1 * len(all_data))\n",
    "\n",
    "train_data = all_data[:train_size]\n",
    "val_data = all_data[train_size:train_size+val_size]\n",
    "test_data = all_data[train_size+val_size:]\n",
    "\n",
    "# Create HuggingFace Dataset\n",
    "final_dataset = DatasetDict({\n",
    "    'train': Dataset.from_list(train_data),\n",
    "    'validation': Dataset.from_list(val_data),\n",
    "    'test': Dataset.from_list(test_data)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff99e876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL DATASET STATISTICS\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'category'],\n",
      "        num_rows: 64000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'category'],\n",
      "        num_rows: 8000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'category'],\n",
      "        num_rows: 8000\n",
      "    })\n",
      "})\n",
      "\n",
      "Label Distribution:\n",
      "label\n",
      "0    32039\n",
      "1    31961\n",
      "Name: count, dtype: int64\n",
      "\n",
      "  0 = Non-Sensitive (Wikipedia)\n",
      "  1 = Sensitive (PII + API Keys)\n",
      "\n",
      "Category Distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "category\n",
       "non_sensitive    32039\n",
       "pii              23996\n",
       "api_key           7965\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FINAL DATASET STATISTICS\")\n",
    "print(final_dataset)\n",
    "\n",
    "import pandas as pd\n",
    "train_df = pd.DataFrame(final_dataset['train'])\n",
    "print(f\"\\nLabel Distribution:\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(f\"\\n  0 = Non-Sensitive (Wikipedia)\")\n",
    "print(f\"  1 = Sensitive (PII + API Keys)\")\n",
    "\n",
    "print(f\"\\nCategory Distribution:\")\n",
    "train_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c84c310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 64000/64000 [00:00<00:00, 1152795.96 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8000/8000 [00:00<00:00, 470444.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8000/8000 [00:00<00:00, 665168.64 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Dataset saved to './pii_dataset'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_path = \"./pii_dataset\"\n",
    "final_dataset.save_to_disk(output_path)\n",
    "print(f\"\\n✓ Dataset saved to '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e759b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
