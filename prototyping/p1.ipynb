{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be0149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets torch scikit-learn faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088a5789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de917a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "random.seed(42)\n",
    "\n",
    "def gen_access_key():\n",
    "    # AWS Access Key IDs often start with 'AKIA' and are ~20 chars total\n",
    "    return \"AKIA\" + ''.join(random.choices(string.ascii_uppercase + string.digits, k=16))\n",
    "\n",
    "def gen_secret_key():\n",
    "    # Secret keys are typically ~40 chars, base64url-like (letters, digits, /+=)\n",
    "    alphabet = string.ascii_letters + string.digits + '/+='\n",
    "    return ''.join(random.choices(alphabet, k=40))\n",
    "\n",
    "# templates for embedding keys into natural text\n",
    "pos_templates = [\n",
    "    \"My AWS Access Key ID is {}. Please add it to the config.\",\n",
    "    \"Use the following secret to authenticate: {}\",\n",
    "    \"Credentials -> AccessKey: {} SecretKey: {}\",\n",
    "    \"I stored the key {} in the credentials file.\",\n",
    "    \"Here's the AWS secret: {} (do not share).\",\n",
    "    \"Found a key: {} - needs rotation.\"\n",
    "]\n",
    "\n",
    "# negative templates (no real keys)\n",
    "neg_templates = [\n",
    "    \"Please check the AWS docs at https://docs.aws.amazon.com/\",\n",
    "    \"Contact me at {} for more info.\",\n",
    "    \"The build passed successfully on commit {}.\",\n",
    "    \"This string {} is a random token but not a secret.\",\n",
    "    \"Environment variable was redacted: [REDACTED].\",\n",
    "    \"We will rotate credentials every 90 days.\"\n",
    "]\n",
    "\n",
    "def gen_dataset(n_positives=500, n_negatives=500):\n",
    "    rows = []\n",
    "\n",
    "    # positives\n",
    "    for _ in range(n_positives):\n",
    "        typ = random.choice([\"access\", \"secret\", \"both\"])\n",
    "        t = random.choice(pos_templates)\n",
    "        \n",
    "        if \"AccessKey:\" in t and \"SecretKey:\" in t:\n",
    "            # This template needs both keys\n",
    "            ak = gen_access_key()\n",
    "            sk = gen_secret_key()\n",
    "            text = t.format(ak, sk)\n",
    "        else:\n",
    "            # single placeholder\n",
    "            if typ == \"access\":\n",
    "                text = t.format(gen_access_key())\n",
    "            elif typ == \"secret\":\n",
    "                text = t.format(gen_secret_key())\n",
    "            else:\n",
    "                # both combined safely into one sentence\n",
    "                text = f\"AccessKey: {gen_access_key()} SecretKey: {gen_secret_key()} - store securely.\"\n",
    "                \n",
    "        rows.append({\"text\": text, \"label\": 1})\n",
    "\n",
    "    # negatives\n",
    "    for _ in range(n_negatives):\n",
    "        template = random.choice(neg_templates)\n",
    "        token = fake.user_name() if \"{}\" in template else \"\"\n",
    "        random_token = ''.join(random.choices(string.ascii_letters + string.digits, k=12))\n",
    "        if \"{}\" in template:\n",
    "            text = template.format(token if random.random() < 0.6 else random_token)\n",
    "        else:\n",
    "            text = template\n",
    "        if random.random() < 0.2:\n",
    "            fake_like_ak = \"BKIA\" + ''.join(random.choices(string.ascii_uppercase + string.digits, k=16))\n",
    "            text = f\"{text} Note: token {fake_like_ak} is not an AWS key.\"\n",
    "        rows.append({\"text\": text, \"label\": 0})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = gen_dataset(n_positives=500, n_negatives=500)\n",
    "    df.to_csv(\"aws_key_synthetic_dataset.csv\", index=False)\n",
    "    print(\"Saved aws_key_synthetic_dataset.csv with shape:\", df.shape)\n",
    "    print(df.head(12).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce6ed580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "o:\\ds_capestone\\Prelims\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:07, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.540300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.132300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.006200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       100\n",
      "           1       1.00      1.00      1.00       100\n",
      "\n",
      "    accuracy                           1.00       200\n",
      "   macro avg       1.00      1.00      1.00       200\n",
      "weighted avg       1.00      1.00      1.00       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# ✅ Load dataset\n",
    "df = pd.read_csv(\"aws_key_synthetic_dataset.csv\")\n",
    "\n",
    "# Split into train/test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"text\"].tolist(), df[\"label\"].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# ✅ Create torch datasets\n",
    "class KeyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = KeyDataset(train_encodings, train_labels)\n",
    "test_dataset = KeyDataset(test_encodings, test_labels)\n",
    "\n",
    "# ✅ Load DistilBERT model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "\n",
    "# ✅ Training arguments (compatible with older transformers versions)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=1,  # keeps only the last checkpoint\n",
    ")\n",
    "\n",
    "# ✅ Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# ✅ Train model\n",
    "trainer.train()\n",
    "\n",
    "# ✅ Evaluate\n",
    "preds = trainer.predict(test_dataset)\n",
    "y_pred = preds.predictions.argmax(-1)\n",
    "print(classification_report(test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff26c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"My AWS Access Key ID is AKIAJ4D7H8G9F2H2L7P1, rotate it soon.\",\n",
    "    \"The secret key is aH8tA6G9uP9f2D5kQ0zH8D1nC5lR9yU0K8XrY2a, do not share!\",\n",
    "    \"AccessKey: AKIAK1J2K3L4M5N6O7P8 SecretKey: aH8tA6G9uP9f2D5kQ0zH8D1nC5lR9yU0K8XrY2a\",\n",
    "    \"Please save the document on your local machine.\",\n",
    "    \"Note: token AKIAZXY1234567890123456 is not an AWS key.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1f2d1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 0, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# detect device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# tokenize\n",
    "inputs = tokenizer(test_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# move inputs to same device as model\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "print(predictions)  # 1 = sensitive, 0 = non-sensitive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42f00cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('aws_key_detector\\\\tokenizer_config.json',\n",
       " 'aws_key_detector\\\\special_tokens_map.json',\n",
       " 'aws_key_detector\\\\vocab.txt',\n",
       " 'aws_key_detector\\\\added_tokens.json',\n",
       " 'aws_key_detector\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"aws_key_detector\")\n",
    "tokenizer.save_pretrained(\"aws_key_detector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "591d184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"aws_key_detector\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"aws_key_detector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e233c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           text  label      type\n",
      "               Phone number: (287)410-3823x7311      1     phone\n",
      "                             SSN is 486-45-4104      1       ssn\n",
      "              The meeting is scheduled for 3 PM      0      none\n",
      "                     Phone number: 579.664.4012      1     phone\n",
      "                 Server logs indicate no errors      0      none\n",
      "                 Server logs indicate no errors      0      none\n",
      "API URL: https://api.example.com?key=bwSjKht15z      1 url_token\n",
      "              The meeting is scheduled for 3 PM      0      none\n",
      "                    Random string: hqNKuwyLivQL      0      none\n",
      "              Call me at 001-680-259-9316x22608      1     phone\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import string\n",
    "\n",
    "fake = Faker()\n",
    "random.seed(42)\n",
    "\n",
    "# Templates for each sensitive type\n",
    "templates = {\n",
    "    \"aws_access\": [\n",
    "        \"My AWS Access Key is {}\",\n",
    "        \"Use this AccessKey for AWS: {}\"\n",
    "    ],\n",
    "    \"aws_secret\": [\n",
    "        \"AWS SecretKey: {}\",\n",
    "        \"Do not share this secret: {}\"\n",
    "    ],\n",
    "    \"email\": [\n",
    "        \"Please contact me at {}\",\n",
    "        \"Send credentials to {}\"\n",
    "    ],\n",
    "    \"ssn\": [\n",
    "        \"Employee SSN: {}\",\n",
    "        \"SSN is {}\"\n",
    "    ],\n",
    "    \"credit_card\": [\n",
    "        \"Credit card number: {}\",\n",
    "        \"Use card {} for payment\"\n",
    "    ],\n",
    "    \"phone\": [\n",
    "        \"Call me at {}\",\n",
    "        \"Phone number: {}\"\n",
    "    ],\n",
    "    \"url_token\": [\n",
    "        \"API URL: {}\",\n",
    "        \"Use this token: {}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Function to generate random secrets\n",
    "def gen_aws_access(): return \"AKIA\" + ''.join(random.choices(string.ascii_uppercase + string.digits, k=16))\n",
    "def gen_aws_secret(): return ''.join(random.choices(string.ascii_letters + string.digits + '/+=', k=40))\n",
    "def gen_credit_card(): return ' '.join([''.join(random.choices(string.digits, k=4)) for _ in range(4)])\n",
    "def gen_url_token(): return f\"https://api.example.com?key={''.join(random.choices(string.ascii_letters + string.digits, k=10))}\"\n",
    "\n",
    "gen_functions = {\n",
    "    \"aws_access\": gen_aws_access,\n",
    "    \"aws_secret\": gen_aws_secret,\n",
    "    \"email\": fake.email,\n",
    "    \"ssn\": lambda: f\"{random.randint(100,999)}-{random.randint(10,99)}-{random.randint(1000,9999)}\",\n",
    "    \"credit_card\": gen_credit_card,\n",
    "    \"phone\": fake.phone_number,\n",
    "    \"url_token\": gen_url_token\n",
    "}\n",
    "\n",
    "# Generate dataset\n",
    "def generate_dataset(n_samples_per_type=200):\n",
    "    rows = []\n",
    "    for label, funcs in templates.items():\n",
    "        for _ in range(n_samples_per_type):\n",
    "            value = gen_functions[label]()\n",
    "            text = random.choice(funcs).format(value)\n",
    "            rows.append({\"text\": text, \"label\": 1, \"type\": label})\n",
    "\n",
    "    # Add negatives (non-sensitive text)\n",
    "    neg_texts = [\n",
    "        \"The meeting is scheduled for 3 PM\",\n",
    "        \"Please review the attached document\",\n",
    "        \"Server logs indicate no errors\",\n",
    "        \"Random string: {}\".format(''.join(random.choices(string.ascii_letters, k=12)))\n",
    "    ]\n",
    "    for _ in range(n_samples_per_type * len(templates)):\n",
    "        text = random.choice(neg_texts)\n",
    "        rows.append({\"text\": text, \"label\": 0, \"type\": \"none\"})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Generate and save\n",
    "df = generate_dataset(200)\n",
    "df.to_csv(\"multi_sensitive_dataset.csv\", index=False)\n",
    "print(df.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4540796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2800, 3)\n",
      "                                text  label   type\n",
      "0   Phone number: (287)410-3823x7311      1  phone\n",
      "1                 SSN is 486-45-4104      1    ssn\n",
      "2  The meeting is scheduled for 3 PM      0   none\n",
      "3         Phone number: 579.664.4012      1  phone\n",
      "4     Server logs indicate no errors      0   none\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='280' max='280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [280/280 00:16, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       254\n",
      "           1       1.00      1.00      1.00       306\n",
      "\n",
      "    accuracy                           1.00       560\n",
      "   macro avg       1.00      1.00      1.00       560\n",
      "weighted avg       1.00      1.00      1.00       560\n",
      "\n",
      "{'text': 'My AWS Access Key ID is AKIAJ4D7H8G9F2H2L7P1', 'contains_sensitive': True, 'aws_access': 'AKIAJ4D7H8G9F2H2L7P1', 'aws_secret': None, 'email': None, 'ssn': None, 'credit_card': None, 'phone': None, 'url_token': None}\n",
      "{'text': 'The document is saved on my local machine', 'contains_sensitive': False, 'aws_access': None, 'aws_secret': None, 'email': None, 'ssn': None, 'credit_card': None, 'phone': None, 'url_token': None}\n",
      "{'text': 'Email: john.doe@example.com', 'contains_sensitive': True, 'aws_access': None, 'aws_secret': None, 'email': 'john.doe@example.com', 'ssn': None, 'credit_card': None, 'phone': None, 'url_token': None}\n",
      "{'text': 'Employee SSN: 123-45-6789', 'contains_sensitive': True, 'aws_access': None, 'aws_secret': None, 'email': None, 'ssn': '123-45-6789', 'credit_card': None, 'phone': '123-45-6789', 'url_token': None}\n",
      "{'text': 'Credit card number: 4111 1111 1111 1111', 'contains_sensitive': True, 'aws_access': None, 'aws_secret': None, 'email': None, 'ssn': None, 'credit_card': '4111 1111 1111 1111', 'phone': '4111 1111 1111 1111', 'url_token': None}\n",
      "{'text': 'Use this API: https://api.example.com?key=abc123', 'contains_sensitive': True, 'aws_access': None, 'aws_secret': None, 'email': None, 'ssn': None, 'credit_card': None, 'phone': None, 'url_token': 'https://api.example.com?key=abc123'}\n",
      "{'text': 'Call me at +1-202-555-0134', 'contains_sensitive': True, 'aws_access': None, 'aws_secret': None, 'email': None, 'ssn': None, 'credit_card': None, 'phone': '+1-202-555-0134', 'url_token': None}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "\n",
    "# -----------------------------\n",
    "# Load multi-type sensitive dataset\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"multi_sensitive_dataset.csv\")  # columns: text, label\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df.head(5))\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare train/test splits\n",
    "# -----------------------------\n",
    "texts = df['text'].tolist()\n",
    "labels = df['label'].tolist()  # 1=sensitive, 0=non-sensitive\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer\n",
    "# -----------------------------\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset class\n",
    "# -----------------------------\n",
    "class KeyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = KeyDataset(train_encodings, train_labels)\n",
    "test_dataset = KeyDataset(test_encodings, test_labels)\n",
    "\n",
    "# -----------------------------\n",
    "# DistilBERT model\n",
    "# -----------------------------\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Training arguments (older-compatible)\n",
    "# -----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=20,\n",
    "    save_total_limit=1,\n",
    "    do_train=True,\n",
    "    do_eval=True\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Trainer\n",
    "# -----------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Train\n",
    "# -----------------------------\n",
    "trainer.train()\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate\n",
    "# -----------------------------\n",
    "preds = trainer.predict(test_dataset)\n",
    "y_pred = preds.predictions.argmax(-1)\n",
    "print(classification_report(test_labels, y_pred))\n",
    "\n",
    "# -----------------------------\n",
    "# Regex patterns for extraction\n",
    "# -----------------------------\n",
    "regex_dict = {\n",
    "    \"aws_access\": r'\\bAKIA[0-9A-Z]{16}\\b',\n",
    "    \"aws_secret\": r'\\b[A-Za-z0-9/+=]{40}\\b',\n",
    "    \"email\": r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "    \"ssn\": r'\\b\\d{3}-\\d{2}-\\d{4}\\b',\n",
    "    \"credit_card\": r'\\b(?:\\d[ -]*?){13,16}\\b',\n",
    "    \"phone\": r'\\+?\\d[\\d\\s-]{7,}\\d',\n",
    "    \"url_token\": r'https?://[^\\s]+'\n",
    "}\n",
    "\n",
    "def extract_sensitive(text):\n",
    "    extracted = {}\n",
    "    for key, pattern in regex_dict.items():\n",
    "        match = re.search(pattern, text)\n",
    "        extracted[key] = match.group() if match else None\n",
    "    return extracted\n",
    "\n",
    "# -----------------------------\n",
    "# Test: Predict + Extract\n",
    "# -----------------------------\n",
    "def predict_and_extract(texts):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    enc = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**enc)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "    results = []\n",
    "    for text, pred in zip(texts, preds):\n",
    "        res = {\"text\": text, \"contains_sensitive\": bool(pred)}\n",
    "        res.update(extract_sensitive(text) if pred==1 else {k: None for k in regex_dict.keys()})\n",
    "        results.append(res)\n",
    "    return results\n",
    "\n",
    "# -----------------------------\n",
    "# Example Test Sentences\n",
    "# -----------------------------\n",
    "test_sentences = [\n",
    "    \"My AWS Access Key ID is AKIAJ4D7H8G9F2H2L7P1\",\n",
    "    \"The document is saved on my local machine\",\n",
    "    \"Email: john.doe@example.com\",\n",
    "    \"Employee SSN: 123-45-6789\",\n",
    "    \"Credit card number: 4111 1111 1111 1111\",\n",
    "    \"Use this API: https://api.example.com?key=abc123\",\n",
    "    \"Call me at +1-202-555-0134\"\n",
    "]\n",
    "\n",
    "results = predict_and_extract(test_sentences)\n",
    "for r in results:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7937fd32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
